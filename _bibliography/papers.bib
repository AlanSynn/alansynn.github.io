---
---

@string{acm = {Association for Computing Machinery,}}
@string{ieee = {Institute of Electrical and Electronics Engineers,}}

@inproceedings{ahn2021nips,
  abbr={NIPS Demo},
  bibtex_show={false},
  title={Protopia AI: Taking on Missing Link in Inference Privacy},
  author={Ahn, Byung Hoon and Synn, DoangJoo and Derkani, Masih and Ebrahimi, Eiman and Esmaeilzadeh, Hadi},
  booktitle={Neural Information Processing Systems Demonstrations [To Appear]},
  year={2021},
  selected={true},
  organization={Neural Information Processing Systems},
  website={https://protopia.ai/neurips2021demo/}
}

@article{synn2021mbs,
  abbr={Submitted},
  bibtex_show={false},
  title={Micro Batch Streaming: Allowing the Training of DNN models Using a large batch size on Small Memory Systems},
  author={Synn, DoangJoo and Piao, XinYu and Park, JuYoung and Kim, Jong-Kook},
  booktitle={Submitted},
  arxiv={2110.12484},
  selected={true},
}

@article{hahn2021fbs,
  abbr={Submitted},
  title={Feature Based Sampling: A Fast and Robust Sampling Method for Tasks that Use Point Cloud Data},
  author={Han, JungWoo and Synn, DoangJoo and Kim, TaeHyung and Jung, HaeCheon and Kim, Jong-Kook},
  booktitle={Submitted},
  selected={true},
}

@inproceedings{synn2021nwt,
  abbr={KIPS},
  bibtex_show={false},
  title={Num Worker Tuner: An Automated Spawn Parameter Tuner for Multi-Processing DataLoaders},
  author={Synn, DoangJoo and Kim, JongKook},
  booktitle={ACK},
  year={2021},
  selected={true},
  abstract={In training a deep learning model, it is crucial to tune various hyperparameters and gain speed and accuracy. Although the hyperparameters that mathematically affect the convergence significantly affect the training speed, the system parameters that affect the host-to-device data set transmission time also occupy a specific part in the overall time acceleration. Therefore, it is important to properly tune and select parameters that can affect the data loader as a system parameter in overall time acceleration. We propose an automated framework called Num Worker Tuner (NWT) to address this problem. This method finds the appropriate number of multi-processing subprocesses through the search space and accelerates the learning through the number of subprocesses. Furthermore, this method allows memory efficiency and speed-up by tuning the system-dependent parameter, the number of multi-process spawns.},
  organization={Korea Information Processing Society}
}

@inproceedings{park2021survey,
  abbr={KIPS},
  bibtex_show={false},
  title={A Survey on the Advancement of Virtualization Technology},
  author={Park, JooYoung and Synn, DoangJoo and Kim, JongKook},
  booktitle={Proceedings of the Korea Information Processing Society Conference},
  pages={12--15},
  year={2021},
  selected={true},
  organization={Korea Information Processing Society}
}

@article{kim2020sg,
  abbr={KIPS},
  bibtex_show={false},
  title={SG-Drop: Faster Skip-Gram by Dropping Context Words},
  author={Kim, DongJae and Synn, DoangJoo and Kim, Jong-Kook},
  journal={KIPS},
  volume={27},
  number={2},
  pages={1014--1017},
  year={2020},
  pdf={http://manuscriptlink-society-file.s3.amazonaws.com/kips/conference/2020fall/presentation/KIPS_C2020B0301.pdf},
  abstract={Many natural language processing (NLP) models utilize pre-trained word embeddings to leverage latent information. One of the most successful word embedding model is the Skip-gram (SG). In this paper, we propose a Skipgram drop (SG-Drop) model, which is a variation of the SG model. The SG-Drop model is designed to reduce training time efficiently. Furthermore, the SG-Drop allows controlling training time with its hyperparameter. It could train word embedding faster than reducing training epochs while better preserving the quality.},
  selected={true},
  publisher={KIPS}
}

@Article{einstein1905photoelectriceffect,
  bibtex_show={true},
  abbr={Ann. Phys.},
  title="{{\"U}ber einen die Erzeugung und Verwandlung des Lichtes betreffenden heuristischen Gesichtspunkt}",
  author={Albert Einstein},
  abstract={This is the abstract text.},
  journal={Ann. Phys.},
  volume={322},
  number={6},
  pages={132--148},
  year={1905},
  doi={10.1002/andp.19053220607},
  award={Albert Einstein receveid the **Nobel Prize in Physics** 1921 *for his services to Theoretical Physics, and especially for his discovery of the law of the photoelectric effect*},
  award_name={Nobel Prize}
}

@book{przibram1967letters,
  bibtex_show={true},
  title={Letters on wave mechanics},
  author={Einstein, Albert and Schr√∂dinger, Erwin and Planck, Max and Lorentz, Hendrik Antoon and Przibram, Karl},
  year={1967},
  publisher={Vision},
  preview={wave-mechanics.gif},
  abbr={Vision}
}
