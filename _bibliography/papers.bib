---
---

@string{acm = {Association for Computing Machinery,}}
@string{ieee = {Institute of Electrical and Electronics Engineers,}}

@inproceedings{yu2025tangible,
  abbr={CHI},
  bibtex_show={false},
  title={Tangible-MakeCode: Bridging Physical Coding Blocks with a Web-Based Programming Interface for Collaborative and Extensible Learning},
  author={Yu, Jin and Garg, Poojita and Synn, DoangJoo and Oh, HyunJoo},
  booktitle={Proceedings of the 2025 CHI Conference on Human Factors in Computing Systems},
  year={2025},
  month={4},
  publisher={ACM},
  address={Yokohama, Japan},
  doi={10.1145/3706598.3713260},
  isbn={979-8-4007-1394-1/25/04},
  selected={true}
}

@article{piao2023enabling,
  abbr={IEEE Access},
  title={Enabling large batch size training for dnn models beyond the memory limit while maintaining performance},
  author={Piao, XinYu and Synn, DoangJoo and Park, JuYoung and Kim, Jong-Kook},
  journal={IEEE Access},
  volume={11},
  pages={102981--102990},
  year={2023},
  publisher={IEEE},
  doi={10.1109/ACCESS.2023.3307778},
  selected={true}
}

@inproceedings{ahn2021nips,
  abbr={NIPS Demo},
  bibtex_show={false},
  title={Protopia AI: Taking on Missing Link in Inference Privacy},
  author={Ahn, Byung Hoon and Synn, DoangJoo and Derkani, Masih and Ebrahimi, Eiman and Esmaeilzadeh, Hadi},
  booktitle={Neural Information Processing Systems Demonstrations},
  year={2021},
  selected={true},
  organization={Neural Information Processing Systems},
  website={https://protopia.ai/neurips2021demo/}
}

@article{synn2021mbs,
  abbr={arXiv},
  title={Micro Batch Streaming: Allowing the Training of DNN models Using a large batch size on Small Memory Systems},
  author={Synn, DoangJoo and Piao, XinYu and Park, JuYoung and Kim, Jong-Kook},
  journal={arXiv},
  year={2021},
  month={11},
  selected={true}
}

@article{han2022fbs,
  abbr={IEEE Access},
  author={Han, Jung-Woo and Synn, DoangJoo and Kim, Tae-Hyeong and Jung, Hae-Cheon and Kim, Jong-Kook},
  journal={IEEE Access},
  title={Feature Based Sampling: A Fast and Robust Sampling Method for Tasks that Use Point Cloud Data},
  year={2022},
  volume={10},
  pages={58062--58070},
  month={4},
  doi={10.1109/ACCESS.2022.3178519},
  selected={true}
}

@inproceedings{synn2021nwt,
  abbr={ACK},
  bibtex_show={false},
  title={Num Worker Tuner: An Automated Spawn Parameter Tuner for Multi-Processing DataLoaders},
  author={Synn, DoangJoo and Kim, Jong-Kook},
  booktitle={Proceedings of ACK 2021},
  year={2021},
  month={10},
  selected={true},
  abstract={In training a deep learning model, it is crucial to tune various hyperparameters and gain speed and accuracy. Although the hyperparameters that mathematically affect the convergence significantly affect the training speed, the system parameters that affect the host-to-device data set transmission time also occupy a specific part in the overall time acceleration. Therefore, it is important to properly tune and select parameters that can affect the data loader as a system parameter in overall time acceleration. We propose an automated framework called Num Worker Tuner (NWT) to address this problem. This method finds the appropriate number of multi-processing subprocesses through the search space and accelerates the learning through the number of subprocesses. Furthermore, this method allows memory efficiency and speed-up by tuning the system-dependent parameter, the number of multi-process spawns.},
  organization={Korea Information Processing Society}
}

@inproceedings{park2021survey,
  abbr={KIPS},
  bibtex_show={false},
  title={A Survey on the Advancement of Virtualization Technology},
  author={Park, JooYoung and Synn, DoangJoo and Kim, Jong-Kook},
  booktitle={Proceedings of Korea Information Processing Society 2021},
  year={2021},
  month={3},
  selected={true},
  organization={Korea Information Processing Society}
}

@article{kim2020sg,
  abbr={KIPS},
  bibtex_show={false},
  title={SG-Drop: Faster Skip-Gram by Dropping Context Words},
  author={Kim, DongJae and Synn, DoangJoo and Kim, Jong-Kook},
  journal={Proceedings of Korea Information Processing Society 2020},
  year={2020},
  month={11},
  pdf={http://manuscriptlink-society-file.s3.amazonaws.com/kips/conference/2020fall/presentation/KIPS_C2020B0301.pdf},
  abstract={Many natural language processing (NLP) models utilize pre-trained word embeddings to leverage latent information. One of the most successful word embedding model is the Skip-gram (SG). In this paper, we propose a Skipgram drop (SG-Drop) model, which is a variation of the SG model. The SG-Drop model is designed to reduce training time efficiently. Furthermore, the SG-Drop allows controlling training time with its hyperparameter. It could train word embedding faster than reducing training epochs while better preserving the quality.},
  selected={true},
  publisher={Korea Information Processing Society}
}

@article{park2022dataloader,
  abbr={arXiv},
  title={Dataloader Parameter Tuner: An Automated Dataloader Parameter Tuner for Deep Learning Models},
  author={Park, JuYoung and Synn, DoangJoo and Piao, XinYu and Kim, Jong-Kook},
  journal={arXiv preprint arXiv:2210.05244},
  year={2022},
  selected={false}
}

@Article{einstein1905photoelectriceffect,
  bibtex_show={true},
  abbr={Ann. Phys.},
  title="{{\"U}ber einen die Erzeugung und Verwandlung des Lichtes betreffenden heuristischen Gesichtspunkt}",
  author={Albert Einstein},
  abstract={This is the abstract text.},
  journal={Ann. Phys.},
  volume={322},
  number={6},
  pages={132--148},
  year={1905},
  doi={10.1002/andp.19053220607},
  award={Albert Einstein receveid the **Nobel Prize in Physics** 1921 *for his services to Theoretical Physics, and especially for his discovery of the law of the photoelectric effect*},
  award_name={Nobel Prize}
}

@book{przibram1967letters,
  bibtex_show={true},
  title={Letters on wave mechanics},
  author={Einstein, Albert and Schr√∂dinger, Erwin and Planck, Max and Lorentz, Hendrik Antoon and Przibram, Karl},
  year={1967},
  publisher={Vision},
  preview={wave-mechanics.gif},
  abbr={Vision}
}
